{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    "\n",
    "This module will get us familair with the basics of deep learning for NLP, in Tensorflow. \n",
    "We'll cover using embeddings, defining models and some of the tools that tensorflow offers us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate\n",
    "Nerual networks come with a lot of boilerplate. Throughout these modules, we'll provde you with most of it so that you can focus on key concepts. As we progress, we'll learn to roll more of it ourselves. \n",
    "\n",
    "For the duration of this notebook, we'll be focused on a single problem, given a number repeesented as a german word, decide if it is even or odd using a nueral network. \n",
    "\n",
    "During each excercise in this notebook, you'll only implement one function, called model . It looks like this\n",
    "```python\n",
    "def model(inputs,labels,params):\n",
    "    '''\n",
    "        Inputs a dict of tensors {\"sequences\":[?,?],\"lengths\":[?]}\n",
    "        labels a tesnor of shape [?] batch size\n",
    "        returns logits [?] batch_size\n",
    "    '''\n",
    "    lengths = inputs[\"lengths\"]\n",
    "    sequences = inputs[\"sequences\"]\n",
    "    #DEEP LEARNING MAGIC\n",
    "```\n",
    "\n",
    "Once you've defined it, you'll run it with the following code\n",
    "```python\n",
    "    estimator = estimator_factory(model,params)\n",
    "    estimator.train(input_fn=input_fn,steps=100000)\n",
    "```\n",
    "\n",
    "### What is that ? \n",
    "\n",
    "Those two lines leverage Tensorflows Estimator API and Dataset API to make your life easy. You define a model that outputs logits and they do things like\n",
    "* Get data and feed it to the model\n",
    "* Calculate the loss and run the backpropogation\n",
    "* Save checkpoints of your model to disk\n",
    "* Visiualize metrics for you in Tensorboard\n",
    "\n",
    "As we progress we'll go into some of those functions to see how we can expand. \n",
    "\n",
    "## What data will go into your model \n",
    "\n",
    "Your model will receive three tensors, \n",
    "The input/sequences consists of sequences of charchter ids that represent a word. \n",
    "The next tensor, lengths, specifies how long each sequence is\n",
    "The final tensor, labels, specifies if each example is odd (1) or even (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from utils.numtoWord import vocab # Our data's vocabulary\n",
    "from utils.dnn.estimatorTools import estimator_factory, input_fn #Magic\n",
    "params = { #Paramaters for our model. You can change these to see what happens\n",
    "    \"max_num\":5000,\n",
    "    \"batch_size\":32,\n",
    "    \"hidden_size\":128,\n",
    "    \"vocab_size\":len(vocab)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputs,labels,params):\n",
    "    '''\n",
    "        Use this template to solve all of the excercises ahead. \n",
    "        Inputs a dict of tensors {\"sequences\":[?,?],\"lengths\":[?]}\n",
    "        labels a tesnor of shape [?] batch size\n",
    "        returns logits [?] batch_size\n",
    "    '''\n",
    "    lengths = inputs[\"lengths\"]\n",
    "    sequences = inputs[\"sequences\"]\n",
    "    '''\n",
    "        YOU DO WORK HERE\n",
    "    '''\n",
    "    return logits\n",
    "estimator = estimator_factory(model,params)\n",
    "estimator.train(input_fn=input_fn,steps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1 - Embeddings\n",
    "The highlight of deep learning for NLP is that we can use vector representations of words or letters. Our inputs are sequences of ids, which we want to convert to sequences of vectors. \n",
    "Your job is to write a model that\n",
    "1. Takes the ids provided in sequence and converts them to vectors with the [embedding_lookup]. (You'll need google foo for this task) (https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) function\n",
    "2. Reduces each sequence by summing it with [reduce_sum](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup) \n",
    "3. Converts the resulting sum into a logit using [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO WORK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1.1 - Bonus questions\n",
    "If that was easy answer the following ? \n",
    "1. Why won't your models loss converge ? \n",
    "2. If you take the average instead of the sum will it help ? \n",
    "3. Try it \n",
    "4. We never specified the vectors values. Where are they coming from ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2 - RNNs\n",
    "This excercise introduces us to the workhourse of NLP, recurrent neural networks and their variants. In particular we'll look at some useful tools tensorflow provides that make working with sequences easier.  \n",
    "Your job is to write a model that\n",
    "1. As before, embeds the input\n",
    "2. Uses tensorflow's [dynamic rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) to process the sequence\n",
    "3. Convert the final state of the rnn to a logit using [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense)\n",
    "\n",
    "If you did it well, your model should converge\n",
    "## Things to consider\n",
    "1. Use a GRUCell instead of RNN or LSTM. It's interface is slightly simpler\n",
    "2. How does your model behave when you specify sequence_lengths vs when you don't specify them. What is happening ? \n",
    "3. What happens if you add another hidden layer between the rnn and logits ?\n",
    "5. Change the paramater max_len to 10000000, this will make words longer. What changes in your models behaviour ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 3 - BiDirectional RNNs\n",
    "A standard RNN reads the input from left to right. It's helpful if it can read the input from left to right and right to left. That's where BiDirectional RNNs come in. \n",
    "It used to be that implementing them was horrible. Now it's easy, just use [bidirectional_dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn). \n",
    "\n",
    "So for excercise 3, repeat excercise two with BiDirectional RNNS. Make sure you read the docs thoroughly. \n",
    "\n",
    "## Bonus\n",
    "1. Try using an LSTM cell now. What changes with the last state? Why is that ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.sparse_softmax_cross_entropy_with_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
