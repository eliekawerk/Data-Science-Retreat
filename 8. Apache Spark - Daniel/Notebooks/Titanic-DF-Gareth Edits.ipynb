{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf metastore_db/*.lck\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Load the train and test sets\n",
    "- Check the schema, the variables have their right types?\n",
    "- If not, how to correctly load the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the inferSchema option as true retains the correct variable types in the dataframe\n",
    "\n",
    "train = sqlc.read.format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"train.csv\")\n",
    "\n",
    "test = sqlc.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex| Age|SibSp|Parch| Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|34.5|    0|    0| 330911| 7.8292| null|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|47.0|    1|    0| 363272|    7.0| null|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|62.0|    0|    0| 240276| 9.6875| null|       Q|\n",
      "|        895|     3|    Wirz, Mr. Albert|  male|27.0|    0|    0| 315154| 8.6625| null|       S|\n",
      "|        896|     3|Hirvonen, Mrs. Al...|female|22.0|    1|    1|3101298|12.2875| null|       S|\n",
      "+-----------+------+--------------------+------+----+-----+-----+-------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "N of train rows = 891\n",
      "N of test rows = 418\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)\n",
    "test.show(5)\n",
    "print(\"N of train rows = \"+str(train.count()))\n",
    "print(\"N of test rows = \"+str(test.count()))\n",
    "\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Explore the features of your dataset\n",
    "- You can use DataFrame's ***describe*** method to get summary statistics\n",
    "    - hint: ***toPandas*** may be useful to ease the manipulation of small dataframes\n",
    "- Are there any ***NaN*** values in your dataset?\n",
    "- If so, define value/values to fill these ***NaN*** values\n",
    "    - hint: ***na*** property of DataFrames provide several methods of handling NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can easily convert spark dataframes to pandas, but when you're using big data across different machines, you need to use spark\n",
    "\n",
    "train_pd = train.toPandas()\n",
    "test_pd = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of nulls in Age variable = 177\n",
      "N of nulls in Age variable = 177\n"
     ]
    }
   ],
   "source": [
    "# find the number of null values in a given column. You can use the functions module in pyspark to get the .isnull function\n",
    "# filter the database where 'Age' is null, and then count the number of rows\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "train_age_nulls = train.filter(F.isnull(train['Age']))\n",
    "count_age_nulls = train_age_nulls.count()\n",
    "print(\"N of nulls in Age variable = \"+str(count_age_nulls))\n",
    "\n",
    "# we can do this same thing as above by using the 'where' clause and calling .isNull() on the selection\n",
    "print('N of nulls in Age variable = '+str(train.where(train['Age'].isNull()).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 177, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Cabin': 687, 'Embarked': 2}\n"
     ]
    }
   ],
   "source": [
    "# Count how many columns have null values\n",
    "# this is a pyspark list enumeration function that puts the results into a dictionary\n",
    "\n",
    "print({col:train.where(train[col].isNull()).count() for col in train.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   77|\n",
      "|    null|    2|\n",
      "|       C|  168|\n",
      "|       S|  644|\n",
      "+--------+-----+\n",
      "\n",
      "S\n"
     ]
    }
   ],
   "source": [
    "# count number of passengers by 'Embarked' variable\n",
    "train.groupby('Embarked').count().show()\n",
    "\n",
    "# make this a list instead, where each row is a tuple\n",
    "embarked_df = train.groupby('Embarked').count().sort(F.desc('count')).take(1)\n",
    "mode_embarked = embarked_df[0].Embarked\n",
    "print(mode_embarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.69911764705882\n",
      "29.69911764705882\n",
      "There are now 0 nulls in this column\n",
      "{'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Cabin': 0, 'Embarked': 0}\n",
      "{'PassengerId': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 1, 'Cabin': 0, 'Embarked': 0}\n"
     ]
    }
   ],
   "source": [
    "# Find out what the mean age is by using my pandas DFs\n",
    "# I can see from this that the mean() function automatically dismisses null values anyway\n",
    "meanage_withnulls = train_pd['Age'].mean()\n",
    "meanage_withoutnulls = train_pd['Age'].dropna().mean()\n",
    "print(meanage_withnulls)\n",
    "print(meanage_withoutnulls)\n",
    "\n",
    "# hold mean age in variable\n",
    "ageMean = round(float(train_pd.Age.mean()),0)\n",
    "\n",
    "# now fill all nan values in Age column with ageMean\n",
    "trainFilled = train.fillna({'Age':ageMean})\n",
    "testFilled = test.fillna({'Age':ageMean})\n",
    "\n",
    "# check results\n",
    "print('There are now '+str(trainFilled.where(trainFilled['Age'].isNull()).count())+' nulls in this column')\n",
    "\n",
    "# we can continue filling in NaNs in other columns also\n",
    "trainFilled = trainFilled.fillna({'Embarked':mode_embarked, 'Cabin':0})\n",
    "testFilled = testFilled.fillna({'Embarked':mode_embarked, 'Cabin':0})\n",
    "\n",
    "print({col:trainFilled.where(trainFilled[col].isNull()).count() for col in trainFilled.columns})\n",
    "print({col:testFilled.where(testFilled[col].isNull()).count() for col in testFilled.columns})\n",
    "\n",
    "# round the Fare column\n",
    "trainFilled = trainFilled.withColumn('Fare', F.round(trainFilled[\"Fare\"], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- How to handle categorical features?\n",
    "    - hint: check the Estimators and Transformers\n",
    "- Assemble all desired features into a Vector using the VectorAssembler Transformer\n",
    "- Make sure to end up with a DataFrame with two columns: ***Survived*** and ***vFeatures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Embarked']\n",
      "['Pclass', 'Age', 'Fare']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# this cell allows me to just add the variables I want as features, and it works out which are strings to be vectorized\n",
    "\n",
    "df = trainFilled\n",
    "\n",
    "# which variables do we want to be features?\n",
    "featurevars = ['Pclass','Sex','Age','Fare','Embarked']\n",
    "featurevars_types = df[featurevars].dtypes\n",
    "\n",
    "# which ones are strings?\n",
    "stringvars = []\n",
    "for i in range(len(featurevars_types)):\n",
    "    if featurevars_types[i][1] == 'string':\n",
    "        stringvars.append(featurevars_types[i][0])\n",
    "\n",
    "# which ones are numeric?\n",
    "numericvars = featurevars\n",
    "for i in range(len(stringvars)):\n",
    "    if stringvars[i] in featurevars:\n",
    "        numericvars.remove(stringvars[i])\n",
    "print(stringvars)\n",
    "print(numericvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>vFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 22.0, 7.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 38.0, 71.3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 26.0, 7.9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 35.0, 53.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 35.0, 8.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 3.0, 30.0, 8.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 54.0, 51.9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 2.0, 21.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 27.0, 11.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 14.0, 30.1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived                                   vFeatures\n",
       "0         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 22.0, 7.3]\n",
       "1         1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 38.0, 71.3]\n",
       "2         1   [0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 26.0, 7.9]\n",
       "3         1  [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 35.0, 53.1]\n",
       "4         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 35.0, 8.1]\n",
       "5         0   [1.0, 0.0, 0.0, 0.0, 1.0, 3.0, 30.0, 8.5]\n",
       "6         0  [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 54.0, 51.9]\n",
       "7         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 2.0, 21.1]\n",
       "8         1  [0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 27.0, 11.1]\n",
       "9         1  [0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 14.0, 30.1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try putting code from previous cell in a for loop\n",
    "# index string variables so we can one hot encode them\n",
    "\n",
    "for i in range(len(stringvars)):\n",
    "    indexer = StringIndexer().setInputCol(stringvars[i]).setOutputCol('{}Index'.format(stringvars[i]))\n",
    "    if i == 0:\n",
    "        indexeddf = indexer.fit(df).transform(df)\n",
    "    else:\n",
    "        indexeddf = indexer.fit(indexeddf).transform(indexeddf)\n",
    "\n",
    "# now one hot encode them\n",
    "cols = indexeddf.columns[-len(stringvars):]\n",
    "for i in range(len(cols)):\n",
    "    encoder = OneHotEncoder().setInputCol(cols[i]).setOutputCol('{}V'.format(cols[i])).setDropLast(False)\n",
    "    if i == 0:\n",
    "        encodeddf = encoder.transform(indexeddf)\n",
    "    else:\n",
    "        encodeddf = encoder.transform(encodeddf)\n",
    "\n",
    "# now create features that are vectorized in one column\n",
    "for i in range(len(cols)):\n",
    "    cols[i] = cols[i]+'V'\n",
    "\n",
    "cols = cols + numericvars\n",
    "\n",
    "assembler = VectorAssembler().setInputCols(cols).setOutputCol('vFeatures')\n",
    "vectorizeddf = assembler.transform(encodeddf)\n",
    "vectorizeddf = vectorizeddf['Survived','vFeatures']\n",
    "#vectorizeddf.show()\n",
    "vectorizeddf.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "- Apply a normalization Estimator of your choice to the ***features*** vector obtained in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>vFeatures</th>\n",
       "      <th>scaledFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 22.0, 7.3]</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 38.0, 71.3]</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, -1.622...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 26.0, 7.9]</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 35.0, 53.1]</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 35.0, 8.1]</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 3.0, 30.0, 8.5]</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, -1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 54.0, 51.9]</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 2.0, 21.1]</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 27.0, 11.1]</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 14.0, 30.1]</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, -1.622...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived                                   vFeatures  \\\n",
       "0         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 22.0, 7.3]   \n",
       "1         1  [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 38.0, 71.3]   \n",
       "2         1   [0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 26.0, 7.9]   \n",
       "3         1  [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 35.0, 53.1]   \n",
       "4         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 35.0, 8.1]   \n",
       "5         0   [1.0, 0.0, 0.0, 0.0, 1.0, 3.0, 30.0, 8.5]   \n",
       "6         0  [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 54.0, 51.9]   \n",
       "7         0   [1.0, 0.0, 1.0, 0.0, 0.0, 3.0, 2.0, 21.1]   \n",
       "8         1  [0.0, 1.0, 1.0, 0.0, 0.0, 3.0, 27.0, 11.1]   \n",
       "9         1  [0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 14.0, 30.1]   \n",
       "\n",
       "                                      scaledFeatures  \n",
       "0  [0.7372810452296833, -0.7372810452296834, 0.61...  \n",
       "1  [-1.354812621329705, 1.354812621329705, -1.622...  \n",
       "2  [-1.354812621329705, 1.354812621329705, 0.6154...  \n",
       "3  [-1.354812621329705, 1.354812621329705, 0.6154...  \n",
       "4  [0.7372810452296833, -0.7372810452296834, 0.61...  \n",
       "5  [0.7372810452296833, -0.7372810452296834, -1.6...  \n",
       "6  [0.7372810452296833, -0.7372810452296834, 0.61...  \n",
       "7  [0.7372810452296833, -0.7372810452296834, 0.61...  \n",
       "8  [-1.354812621329705, 1.354812621329705, 0.6154...  \n",
       "9  [-1.354812621329705, 1.354812621329705, -1.622...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().setInputCol('vFeatures').setOutputCol('scaledFeatures').setWithStd(True).setWithMean(True)\n",
    "scalerModel = scaler.fit(vectorizeddf).transform(vectorizeddf)\n",
    "scalerModel.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Instead of doing transformations on separate steps, put everything together with a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>scaledFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, -1.622...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, -1.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7372810452296833, -0.7372810452296834, 0.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, 0.6154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>[-1.354812621329705, 1.354812621329705, -1.622...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived                                     scaledFeatures\n",
       "0         0  [0.7372810452296833, -0.7372810452296834, 0.61...\n",
       "1         1  [-1.354812621329705, 1.354812621329705, -1.622...\n",
       "2         1  [-1.354812621329705, 1.354812621329705, 0.6154...\n",
       "3         1  [-1.354812621329705, 1.354812621329705, 0.6154...\n",
       "4         0  [0.7372810452296833, -0.7372810452296834, 0.61...\n",
       "5         0  [0.7372810452296833, -0.7372810452296834, -1.6...\n",
       "6         0  [0.7372810452296833, -0.7372810452296834, 0.61...\n",
       "7         0  [0.7372810452296833, -0.7372810452296834, 0.61...\n",
       "8         1  [-1.354812621329705, 1.354812621329705, 0.6154...\n",
       "9         1  [-1.354812621329705, 1.354812621329705, -1.622..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "indexer = StringIndexer().setInputCol('Sex').setOutputCol('SexIndex')\n",
    "indexer2 = StringIndexer().setInputCol('Embarked').setOutputCol('EmbarkedIndex')\n",
    "encoder = OneHotEncoder().setInputCol('SexIndex').setOutputCol('SexIndexV').setDropLast(False)\n",
    "encoder2 = OneHotEncoder().setInputCol('EmbarkedIndex').setOutputCol('EmbarkedIndexV').setDropLast(False)\n",
    "assembler = VectorAssembler().setInputCols(['SexIndexV','EmbarkedIndexV','Pclass','Age','Fare']).setOutputCol('vFeatures')\n",
    "scaler = StandardScaler().setInputCol('vFeatures').setOutputCol('scaledFeatures').setWithStd(True).setWithMean(True)\n",
    "\n",
    "pipeline = Pipeline().setStages([indexer,indexer2,encoder,encoder2,assembler,scaler])\n",
    "pipelineModel = pipeline.fit(trainFilled)\n",
    "model = pipelineModel.transform(trainFilled)\n",
    "model = model['Survived','scaledFeatures']\n",
    "model.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "- Train a classifier of your choice (for instance, Random Forest) using your dataset of LabeledPoints\n",
    "- Make predictions for the training data\n",
    "- Use the evaluators to find the Area Under ROC and Accuracy of your model\n",
    "- How is your model performing? Try to tune its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|      scaledFeatures|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|[0.73728104522968...|[90.291677539239,...|[0.90291677539239...|       0.0|\n",
      "|       1|[-1.3548126213297...|[2.05747820498528...|[0.02057478204985...|       1.0|\n",
      "|       1|[-1.3548126213297...|[53.8465428305998...|[0.53846542830599...|       0.0|\n",
      "|       1|[-1.3548126213297...|[3.96761464748734...|[0.03967614647487...|       1.0|\n",
      "|       0|[0.73728104522968...|[89.9860679848828...|[0.89986067984882...|       0.0|\n",
      "|       0|[0.73728104522968...|[89.2719053157988...|[0.89271905315798...|       0.0|\n",
      "|       0|[0.73728104522968...|[70.0919721637712...|[0.70091972163771...|       0.0|\n",
      "|       0|[0.73728104522968...|[57.2963772601435...|[0.57296377260143...|       0.0|\n",
      "|       1|[-1.3548126213297...|[49.6961446300845...|[0.49696144630084...|       1.0|\n",
      "|       1|[-1.3548126213297...|[4.99462863700698...|[0.04994628637006...|       1.0|\n",
      "+--------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Area under ROC = 0.901\n",
      "Accuracy = 0.844\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "rfC = RandomForestClassifier().setLabelCol('Survived').setFeaturesCol('scaledFeatures').setNumTrees(100)\n",
    "predict_rfC = rfC.fit(model).transform(model)\n",
    "predict_rfC.show(10)\n",
    "\n",
    "# something that's strange about pyspark is that you can only get Area Under ROC as a metric out of the binaryevaluator\n",
    "# to get accuracy, even for a binary problem, you have to build a multipleclassificationevaluator, and then call the accuracy metric from it\n",
    "\n",
    "binaryevaluator = BinaryClassificationEvaluator().setLabelCol('Survived')\\\n",
    ".setRawPredictionCol('rawPrediction')\\\n",
    ".setMetricName('areaUnderROC')\n",
    "bievaluated_rfC = binaryevaluator.evaluate(predict_rfC)\n",
    "print('Area under ROC = '+str(round(bievaluated_rfC,3)))\n",
    "\n",
    "# so then use multiclass evaluator here to get accuracy\n",
    "multievaluator = MulticlassClassificationEvaluator().setLabelCol('Survived')\\\n",
    ".setMetricName('accuracy')\n",
    "multievaluated_rfC = multievaluator.evaluate(predict_rfC)\n",
    "print('Accuracy = '+str(round(multievaluated_rfC,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "- Take a look at the test data - use DataFrame's ***createOrReplaceTempView*** method to perform SQL queries over the data\n",
    "    - hint: check if there are any NULL values in the dataset - if so, handle them\n",
    "- Apply the transformations to the test data\n",
    "    - hint: include the model to the pipeline\n",
    "- Make predictions using the model previously trained and the transformed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "\n",
    "- Load the answers for the ***test*** data\n",
    "- Combine it with your predictions into a single DataFrame\n",
    "- Use the evaluator you created on ***Step 6***\n",
    "- What was your score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
